{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# ì—‘ì…€ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "file_path = \"../data/NewspaperChurn.xlsx\"\n",
    "df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns drop\n",
    "df = df.drop(['SubscriptionID', 'Ethnicity', 'Language', 'Address', 'State', 'City', 'County', 'Zip Code', 'Source Channel'], axis=1)\n",
    "\n",
    "# Subscriberì—´ No : 1, Yes :0\n",
    "def preprocess_subscriber(df):\n",
    "    df['Subscriber'] = df['Subscriber'].map({'NO': 1, 'YES': 0})\n",
    "    return df\n",
    "    \n",
    "# reward program ì—´ 1 ì´ìƒì€ 1, ë¯¸ë§Œì€ 0, ê²°ì¸¡ì¹˜ë„ 0\n",
    "def preprocess_reward_program(df):\n",
    "    df['reward program'] = df['reward program'].apply(lambda x: 1 if x >= 1 else 0)\n",
    "    return df\n",
    "    \n",
    "# ê³ ê°ìœ í˜•ì½”ë“œ\n",
    "def preprocess_nielsen_prizm(df):\n",
    "    nielsen_mapping = {\n",
    "        'FM': 0,  # Female Middle-aged\n",
    "        'MW': 1,  # Male Working-age\n",
    "        'MM': 2,  # Male Middle-aged\n",
    "        'FW': 3,  # Female Working-age\n",
    "        'YW': 4,  # Young Woman\n",
    "        'YM': 5,  # Young Man\n",
    "        'ME': 6,  # Male Elderly\n",
    "        'FE': 7,  # Female Elderly\n",
    "        'YE': 8   # Young Elderly\n",
    "    }\n",
    "    \n",
    "    # 'Nielsen Prizm'ì„ ë§¤í•‘ëœ ìˆ«ìë¡œ ë³€í™˜\n",
    "    df['Nielsen Prizm'] = df['Nielsen Prizm'].map(nielsen_mapping)\n",
    "    \n",
    "    # 129ê°œì˜ ê²°ì¸¡ì¹˜ëŠ” ë‚ ë¦¼ (ì‚­ì œ) # ì´ê±°ëŠ” ë‚˜ì¤‘ì— ì‚­ì œ ?? # ìœ„ì—ì„œ ë”°ë¡œ\n",
    "    df.dropna(subset=['Nielsen Prizm'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# 2. ê·¼ë¡œ ì¤‘ ì—¬ë¶€ (ì¤‘ì´ë©´ 1, ì•„ë‹ˆë©´ 0)\n",
    "def preprocess_working(df):\n",
    "    df['Working'] = df['Nielsen Prizm'].apply(lambda x: 1 if x in [1, 3] else 0)\n",
    "    return df\n",
    "\n",
    "# 3. ì„±ë³„ (ë‚¨ : 0, ì—¬: 1)\n",
    "def preprocess_gender(df):\n",
    "    # ê¸°ë³¸ ì„±ë³„ ì²˜ë¦¬\n",
    "    df['Gender'] = df['Nielsen Prizm'].apply(lambda x: 0 if x in [1, 2, 5, 6] else 1)\n",
    "    \n",
    "    # 'YE' (Young Elderly) ì²˜ë¦¬: 25ëª… ë‚¨ì„± (0), 26ëª… ì—¬ì„± (1) ëœë¤ìœ¼ë¡œ ë°°ë¶„\n",
    "    ye_indices = df[df['Nielsen Prizm'] == 8].index  # 'YE'ì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ ì°¾ê¸°\n",
    "    \n",
    "    # ëœë¤ ì‹œë“œë¥¼ ê³ ì •í•˜ì—¬ í•­ìƒ ë™ì¼í•œ ê²°ê³¼ê°€ ë‚˜ì˜¤ë„ë¡ ì„¤ì •\n",
    "    random.seed(42)  # ì‹œë“œ ê°’ (ê³ ì •ëœ ìˆ«ì)\n",
    "    \n",
    "    # 'YE'ì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ì˜ ê¸¸ì´ì— ë§ëŠ” gender_assignment ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "    gender_assignment = [0] * (len(ye_indices) // 2) + [1] * (len(ye_indices) - len(ye_indices) // 2)  # ë‚¨ì„± 25ëª…, ì—¬ì„± 26ëª…\n",
    "    random.shuffle(gender_assignment)  # ì„ê¸°\n",
    "    \n",
    "    # YE ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ì„±ë³„ ê°’ì„ í• ë‹¹\n",
    "    df.loc[ye_indices, 'Gender'] = gender_assignment\n",
    "    \n",
    "    return df\n",
    "\n",
    "# weekly fee\n",
    "def preprocess_fee(value):\n",
    "    if pd.isnull(value):\n",
    "        return None\n",
    "    \n",
    "    match = re.findall(r'\\d+\\.\\d+|\\d+', str(value))\n",
    "    num = [float(x) for x in match]\n",
    "\n",
    "    if len(num) == 2:\n",
    "        return (num[0] + num[1]) / 2\n",
    "    elif len(num) == 1:\n",
    "        return num[0]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def nan_fee(value):\n",
    "    avg_fee = np.mean(df['weekly fee'])\n",
    "    if pd.isnull(value):\n",
    "        return avg_fee\n",
    "    else:\n",
    "        return value\n",
    "        \n",
    "# Deliveryperiod ì „ì²˜ë¦¬\n",
    "def preprocess_deliveryperiod(df):\n",
    "    # ì˜¨ë¼ì¸ ë°°ì†¡ íƒ€ì… ì •ì˜\n",
    "    online = {\n",
    "        '7DayT', 'Fri-SunT', 'Sun-FriT', 'Thu-SunT', \n",
    "        'SoooTFST', 'Fri-SunT', 'SooooFST', 'SoooooST', 'SunOnlyT', 'SooooooT'\n",
    "    }\n",
    "\n",
    "\n",
    "    period_map = {\n",
    "        '7Day': 7, '7DAY': 7, '7day': 7, '7DayOL': 7, '7DayT': 7,\n",
    "        'Fri-Sun': 6, 'Fri-SunT': 6, 'Sun-Fri': 6, 'Sun-FriT': 6,\n",
    "        'oMTWTFo': 5, 'Mon-Fri': 5,\n",
    "        'Thu-Sun': 4, 'THU-SUN': 4, 'thu-sun': 4, 'SoooTFS': 4, 'SoooTFST': 4, 'Thu-SunT': 4,\n",
    "        'SooooFS': 3, 'SooooFST': 3, 'Fri-SunT': 3, 'Fri-Sun': 3,\n",
    "        'SatSun': 2, 'SoooooS': 2, 'SoooooST': 2,\n",
    "        'SunOnly': 1, 'SunOnlyT': 1, 'sunonly': 1, 'SUNONLY': 1, 'Soooooo': 1, 'SooooooT': 1\n",
    "    }\n",
    "\n",
    "    df['Is_Online'] = df['Deliveryperiod'].apply(lambda x: 1 if x in online else 0)\n",
    "    df['Deliveryperiod'] = df['Deliveryperiod'].map(period_map)\n",
    "\n",
    "    return df\n",
    "\n",
    "            \n",
    "# dummy for children (ìë…€ê°€ ìˆìœ¼ë©´ 1, ì—†ìœ¼ë©´ 0)       \n",
    "def preprocess_dummy(df):\n",
    "    df['dummy for Children'] = np.where(df['dummy for Children'] == 'Y', 1, 0)\n",
    "    return df\n",
    "    \n",
    "    \n",
    "#hh income ì „ì²˜ë¦¬\n",
    "def preprocess_hh_income(df):\n",
    "    \"\"\"\n",
    "    HH Income ì—´ì„ ì „ì²˜ë¦¬í•˜ì—¬:\n",
    "    1. `plus`ë‚˜ `under`ê°€ í¬í•¨ëœ ê°’ì„ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´\n",
    "    2. ë²”ìœ„ ë°ì´í„°(`$30,000 - $39,999`)ëŠ” í‰ê· ê°’ìœ¼ë¡œ ë³€í™˜\n",
    "    3. ê¸°í˜¸(`$`, `,`) ì œê±°\n",
    "    \"\"\"\n",
    "    # 1. \"plus\" ë˜ëŠ” \"under\"ê°€ í¬í•¨ëœ í–‰ì„ ë§ˆìŠ¤í‚¹\n",
    "    mask = df['HH Income'].str.contains('plus|under', case=False, na=False)\n",
    "    \n",
    "    # 2. ë²”ìœ„ ë°ì´í„°ë¥¼ í‰ê· ê°’ìœ¼ë¡œ ë³€í™˜ (plus/under ì œì™¸)\n",
    "    def process_value(value):\n",
    "        try:\n",
    "            # ê¸°í˜¸ ì œê±°\n",
    "            clean_value = str(value).replace('$', '').replace(',', '')\n",
    "            \n",
    "            # ë²”ìœ„ ì²˜ë¦¬ (ì˜ˆ: \"30000-39999\")\n",
    "            if '-' in clean_value:\n",
    "                low, high = map(int, clean_value.split('-'))\n",
    "                return (low + high) / 2\n",
    "            # ìˆ«ì ì²˜ë¦¬\n",
    "            elif clean_value.isdigit():\n",
    "                return float(clean_value)\n",
    "            else:\n",
    "                return np.nan  # plus/underëŠ” NaN ì²˜ë¦¬\n",
    "        except:\n",
    "            return np.nan\n",
    "    \n",
    "    # 3. ì „ì²´ ì—´ì„ ìˆ«ìë¡œ ë³€í™˜ (plus/underëŠ” NaN)\n",
    "    df['HH Income_Processed'] = df['HH Income'].apply(process_value)\n",
    "    \n",
    "    # 4. í‰ê· ê°’ ê³„ì‚° (plus/under ì œì™¸)\n",
    "    valid_mean = df.loc[~mask, 'HH Income_Processed'].mean()\n",
    "    \n",
    "    # 5. plus/underê°€ í¬í•¨ëœ í–‰ì„ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´\n",
    "    df.loc[mask, 'HH Income_Processed'] = valid_mean\n",
    "    \n",
    "    # 6. ì›ë³¸ ì—´ ëŒ€ì²´\n",
    "    df['HH Income'] = df['HH Income_Processed'].round(0).astype(int)\n",
    "    df = df.drop('HH Income_Processed', axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "#Home Ownership ì „ì²˜ë¦¬\n",
    "def preprocess_home_ownership(df):\n",
    "    df['Home Ownership'] = df['Home Ownership'].map({'OWNER': 1, 'RENTER': 0})\n",
    "    return df\n",
    "\n",
    "def preprocess_age(value):\n",
    "    if pd.isnull(value):\n",
    "        return None\n",
    "    \n",
    "    match = re.findall(r'\\d[\\d,]*', str(value))\n",
    "    num = [int(x.replace(',', '')) for x in match]\n",
    "\n",
    "    if len(num) == 2:\n",
    "        return (num[0] + num[1]) / 2\n",
    "    elif len(num) == 1:\n",
    "        return num[0]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def nan_age(value):\n",
    "    avg_age = np.mean(df['Age range'])\n",
    "    if pd.isnull(value):\n",
    "        return avg_age\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "df['Age range'] = df['Age range'].apply(preprocess_age)\n",
    "df['Age range'] = df['Age range'].apply(nan_age)\n",
    "df['Age range'] = np.floor(df['Age range']).astype(int)\n",
    "df.rename(columns={'Age range': 'Age'}, inplace=True)\n",
    "df = preprocess_subscriber(df)\n",
    "df = preprocess_reward_program(df)\n",
    "df = preprocess_nielsen_prizm(df)\n",
    "df = preprocess_working(df)\n",
    "df = preprocess_gender(df)\n",
    "df['weekly fee'] = df['weekly fee'].apply(preprocess_fee)\n",
    "df['weekly fee'] = df['weekly fee'].apply(nan_fee)\n",
    "df = preprocess_deliveryperiod(df)\n",
    "df = preprocess_dummy(df)\n",
    "df = preprocess_hh_income(df)\n",
    "df = preprocess_home_ownership(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### íƒ€ê¹ƒ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['Subscriber']\n",
    "df = df.drop('Subscriber', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ëª¨ë“  ëª¨ë¸ í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ë¡œì§€ìŠ¤í‹± íšŒê·€ (Cost-Sensitive)]\n",
      "Accuracy: 0.6809\n",
      "Precision: 0.8771\n",
      "Recall: 0.7084\n",
      "F1 Score: 0.7838\n",
      "AUC-ROC: 0.6333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.56      0.39       577\n",
      "           1       0.88      0.71      0.78      2569\n",
      "\n",
      "    accuracy                           0.68      3146\n",
      "   macro avg       0.59      0.63      0.59      3146\n",
      "weighted avg       0.77      0.68      0.71      3146\n",
      "\n",
      "\n",
      "[KNN]\n",
      "Accuracy: 0.6771\n",
      "Precision: 0.8916\n",
      "Recall: 0.6882\n",
      "F1 Score: 0.7768\n",
      "AUC-ROC: 0.6578\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.63      0.42       577\n",
      "           1       0.89      0.69      0.78      2569\n",
      "\n",
      "    accuracy                           0.68      3146\n",
      "   macro avg       0.60      0.66      0.60      3146\n",
      "weighted avg       0.79      0.68      0.71      3146\n",
      "\n",
      "\n",
      "[ëœë¤ í¬ë ˆìŠ¤íŠ¸ (Cost-Sensitive)]\n",
      "Accuracy: 0.8147\n",
      "Precision: 0.8796\n",
      "Recall: 0.8957\n",
      "F1 Score: 0.8876\n",
      "AUC-ROC: 0.6749\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.45      0.47       577\n",
      "           1       0.88      0.90      0.89      2569\n",
      "\n",
      "    accuracy                           0.81      3146\n",
      "   macro avg       0.69      0.67      0.68      3146\n",
      "weighted avg       0.81      0.81      0.81      3146\n",
      "\n",
      "\n",
      "[XGBoost (Cost-Sensitive)]\n",
      "Accuracy: 0.8433\n",
      "Precision: 0.8460\n",
      "Recall: 0.9879\n",
      "F1 Score: 0.9115\n",
      "AUC-ROC: 0.5936\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.20      0.32       577\n",
      "           1       0.85      0.99      0.91      2569\n",
      "\n",
      "    accuracy                           0.84      3146\n",
      "   macro avg       0.82      0.59      0.61      3146\n",
      "weighted avg       0.84      0.84      0.80      3146\n",
      "\n",
      "\n",
      "[ë‹¤ì¤‘ í¼ì…‰íŠ¸ë¡  (MLP, Cost-Sensitive)]\n",
      "Accuracy: 0.7285\n",
      "Precision: 0.8830\n",
      "Recall: 0.7696\n",
      "F1 Score: 0.8224\n",
      "AUC-ROC: 0.6577\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.55      0.42       577\n",
      "           1       0.88      0.77      0.82      2569\n",
      "\n",
      "    accuracy                           0.73      3146\n",
      "   macro avg       0.62      0.66      0.62      3146\n",
      "weighted avg       0.78      0.73      0.75      3146\n",
      "\n",
      "\n",
      "[SVM (Cost-Sensitive)]\n",
      "Accuracy: 0.6964\n",
      "Precision: 0.8895\n",
      "Recall: 0.7174\n",
      "F1 Score: 0.7942\n",
      "AUC-ROC: 0.6603\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.60      0.42       577\n",
      "           1       0.89      0.72      0.79      2569\n",
      "\n",
      "    accuracy                           0.70      3146\n",
      "   macro avg       0.61      0.66      0.61      3146\n",
      "weighted avg       0.79      0.70      0.73      3146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# ë°ì´í„° ë¶„ë¦¬\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, target, test_size=0.2, random_state=0)\n",
    "\n",
    "# 1. ë°ì´í„° ì •ê·œí™”\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Smote\n",
    "smote_tomek = SMOTE(random_state=0)\n",
    "X_train_scaled, y_train= smote_tomek.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# 3. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ í•¨ìˆ˜ ì •ì˜\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"AUC-ROC: {roc_auc:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 4. ë¡œì§€ìŠ¤í‹± íšŒê·€ (ê°€ì¤‘ì¹˜ ì¶”ê°€)\n",
    "print(\"\\n[ë¡œì§€ìŠ¤í‹± íšŒê·€ (Cost-Sensitive)]\")\n",
    "lr_model = LogisticRegression(random_state=0, class_weight='balanced')\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "evaluate_model(lr_model, X_test_scaled, y_test)\n",
    "\n",
    "# 5. KNN ëª¨ë¸\n",
    "print(\"\\n[KNN]\")\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train_scaled, y_train)\n",
    "evaluate_model(knn_model, X_test_scaled, y_test)\n",
    "\n",
    "# 6. ëœë¤ í¬ë ˆìŠ¤íŠ¸ (ê°€ì¤‘ì¹˜ ì¶”ê°€)\n",
    "print(\"\\n[ëœë¤ í¬ë ˆìŠ¤íŠ¸ (Cost-Sensitive)]\")\n",
    "rf_model = RandomForestClassifier(random_state=0, class_weight='balanced')\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "evaluate_model(rf_model, X_test_scaled, y_test)\n",
    "\n",
    "# 7. XGBoost ëª¨ë¸ (ê°€ì¤‘ì¹˜ ì¶”ê°€)\n",
    "print(\"\\n[XGBoost (Cost-Sensitive)]\")\n",
    "xgb_model = XGBClassifier(random_state=0, eval_metric='logloss', scale_pos_weight=4)\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "evaluate_model(xgb_model, X_test_scaled, y_test)\n",
    "\n",
    "# 8. ë‹¤ì¤‘ í¼ì…‰íŠ¸ë¡  (MLPClassifier, ê°€ì¤‘ì¹˜ ì¶”ê°€)\n",
    "print(\"\\n[ë‹¤ì¤‘ í¼ì…‰íŠ¸ë¡  (MLP, Cost-Sensitive)]\")\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=300, random_state=0)\n",
    "mlp_model.fit(X_train_scaled, y_train)\n",
    "evaluate_model(mlp_model, X_test_scaled, y_test)\n",
    "\n",
    "# 9. SVM (ê°€ì¤‘ì¹˜ ì¶”ê°€)\n",
    "print(\"\\n[SVM (Cost-Sensitive)]\")\n",
    "svm_model = SVC(kernel='rbf', probability=True, class_weight='balanced', random_state=0)\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "evaluate_model(svm_model, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP íŠœë‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# ë°ì´í„° ë¶„ë¦¬\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, target, test_size=0.2, random_state=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_resample_train_scaled = scaler.fit_transform(X_train)\n",
    "X_resample_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resample, y_resample = smote.fit_resample(X_resample_train_scaled, y_train)\n",
    "\n",
    "# MLPClassifierë¥¼ ìœ„í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(64, 32, 16), (128, 64, 32), (32, 16, 8)],  # ì€ë‹‰ì¸µ êµ¬ì¡° ë‹¤ì–‘í™”\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],   \n",
    "    'learning_rate_init': [0.001, 0.005, 0.01],  # í•™ìŠµë¥  íŠœë‹\n",
    "    'alpha': [0.0001, 0.001, 0.01],  # ì •ê·œí™” ê°•ë„\n",
    "    'batch_size': [32, 64, 128],  # ë¯¸ë‹ˆ ë°°ì¹˜ í¬ê¸° ì¡°ì •\n",
    "    'max_iter': [500, 1000],  # ì—í¬í¬ ì¦ê°€\n",
    "}\n",
    "\n",
    "# ëª¨ë¸ ì •ì˜\n",
    "mlp = MLPClassifier(early_stopping=True, random_state=0)\n",
    "\n",
    "# ê·¸ë¦¬ë“œ ì„œì¹˜ ì„¤ì •\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "grid_search = GridSearchCV(mlp, param_grid, scoring='f1_weighted', cv=cv, n_jobs=-1, verbose=2)\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ ë° ìµœì  íŒŒë¼ë¯¸í„° íƒìƒ‰\n",
    "grid_search.fit(X_resample, y_resample)\n",
    "\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best F1-Score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# ìµœì  ëª¨ë¸ë¡œ ì˜ˆì¸¡\n",
    "best_mlp = grid_search.best_estimator_\n",
    "y_pred = best_mlp.predict(X_resample_test_scaled)\n",
    "\n",
    "# í‰ê°€\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM íŠœë‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# ë°ì´í„° ë¶„ë¦¬\n",
    "X_train, X_test, y_train, y_test = train_test_split(df,target, test_size=0.2, random_state=0)\n",
    "\n",
    "# ë°ì´í„° ìŠ¤ì¼€ì¼ë§\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# SVM í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],               # ê·œì œ ê°•ë„\n",
    "    'kernel': ['linear', 'rbf', 'poly'],  # ì»¤ë„ í•¨ìˆ˜\n",
    "    'gamma': ['scale', 'auto'],           # ê°ë§ˆ ì„¤ì •\n",
    "    'degree': [2, 3],                     # ë‹¤í•­ ì»¤ë„ ì°¨ìˆ˜\n",
    "}\n",
    "\n",
    "# ëª¨ë¸ ì •ì˜\n",
    "svc = SVC(random_state=0)\n",
    "\n",
    "# ê·¸ë¦¬ë“œ ì„œì¹˜ ì„¤ì •\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "grid_search = GridSearchCV(svc, param_grid, scoring='f1_weighted', cv=cv, n_jobs=-1, verbose=2)\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ ë° ìµœì  íŒŒë¼ë¯¸í„° íƒìƒ‰\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best F1-Score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# ìµœì  ëª¨ë¸ë¡œ ì˜ˆì¸¡\n",
    "best_svc = grid_search.best_estimator_\n",
    "y_pred = best_svc.predict(X_test_scaled)\n",
    "\n",
    "# í‰ê°€\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting íŠœë‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# ë°ì´í„° ë¶„ë¦¬\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# ë°ì´í„° ìŠ¤ì¼€ì¼ë§\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# SMOTEë¡œ ë°ì´í„° ì¦ê°•\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resample, y_resample = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# 1. ê¸°ë³¸ Gradient Boosting ëª¨ë¸\n",
    "gbc = GradientBoostingClassifier(random_state=42)\n",
    "gbc.fit(X_resample, y_resample)\n",
    "y_pred_gbc = gbc.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\n[Gradient Boosting ê¸°ë³¸ ëª¨ë¸ í‰ê°€]\")\n",
    "print(\"ì •í™•ë„:\", accuracy_score(y_test, y_pred_gbc))\n",
    "print(classification_report(y_test, y_pred_gbc))\n",
    "\n",
    "# 2. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (GridSearchCV)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(GradientBoostingClassifier(random_state=42),\n",
    "                           param_grid, scoring='f1_weighted', cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_resample, y_resample)\n",
    "\n",
    "print(\"\\n[Gradient Boosting ìµœì  íŒŒë¼ë¯¸í„°]\")\n",
    "print(grid_search.best_params_)\n",
    "best_gbc = grid_search.best_estimator_\n",
    "\n",
    "# ìµœì  ëª¨ë¸ë¡œ ì˜ˆì¸¡\n",
    "y_pred_best_gbc = best_gbc.predict(X_test_scaled)\n",
    "print(\"\\n[ìµœì  Gradient Boosting ëª¨ë¸ í‰ê°€]\")\n",
    "print(\"ì •í™•ë„:\", accuracy_score(y_test, y_pred_best_gbc))\n",
    "print(classification_report(y_test, y_pred_best_gbc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM íŠœë‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, target, test_size=0.2, random_state=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_resample_train_scaled = scaler.fit_transform(X_train)\n",
    "X_resample_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_resample_train_scaled, y_train)  # Fix: Resample both X_train and y_train together\n",
    "\n",
    "\n",
    "# 4. LightGBM ëª¨ë¸ ì„¤ì •\n",
    "lgbm = lgb.LGBMClassifier(random_state=0, is_unbalance=True)\n",
    "\n",
    "# 5. í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì„¤ì •\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],        # íŠ¸ë¦¬ ê°œìˆ˜\n",
    "    'learning_rate': [0.01, 0.05, 0.1],     # í•™ìŠµë¥ \n",
    "    'num_leaves': [31, 63, 127],            # ë¦¬í”„ ë…¸ë“œ ìˆ˜\n",
    "    'max_depth': [-1, 10, 20],              # íŠ¸ë¦¬ ê¹Šì´\n",
    "    'scale_pos_weight': [1, 10, 20]         # ë¶ˆê· í˜• ë°ì´í„° ë³´ì •\n",
    "}\n",
    "\n",
    "# 6. êµì°¨ ê²€ì¦ ì„¤ì • (Stratified K-Fold ì‚¬ìš©)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# 7. ê·¸ë¦¬ë“œ ì„œì¹˜ ì„¤ì •\n",
    "grid_search = GridSearchCV(\n",
    "    lgbm, param_grid, scoring='f1_weighted', \n",
    "    cv=cv, n_jobs=-1, verbose=2\n",
    ")\n",
    "\n",
    "# 8. ëª¨ë¸ í•™ìŠµ\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 10. ìµœì  ëª¨ë¸ë¡œ ì˜ˆì¸¡\n",
    "best_lgbm = grid_search.best_estimator_\n",
    "y_pred = best_lgbm.predict(X_resample_test_scaled)\n",
    "\n",
    "# 11. ëª¨ë¸ í‰ê°€\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN íŠœë‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt\n",
    "import hyperopt.hp\n",
    "\n",
    "def objective_knn(params):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    model = KNeighborsClassifier(\n",
    "        n_neighbors=int(params['n_neighbors']),\n",
    "        weights=params['weights'],\n",
    "        metric=params['metric']\n",
    "    )\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return {'loss': -acc, 'status': hyperopt.STATUS_OK}\n",
    "\n",
    "space = {\n",
    "    'n_neighbors': hp.quniform('n_neighbors', 3, 20, 1),\n",
    "    'weights': hp.choice('weights', ['uniform', 'distance']),\n",
    "    'metric': hp.choice('metric', ['euclidean', 'manhattan']),\n",
    "    'algorithm': hp.choice('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute'])\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(\n",
    "    fn=objective_knn,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "weights_options = ['uniform', 'distance']\n",
    "metric_options = ['euclidean', 'manhattan']\n",
    "\n",
    "best['n_neighbors'] = int(best['n_neighbors'])\n",
    "best['weights'] = weights_options[best['weights']]\n",
    "best['metric'] = metric_options[best['metric']]\n",
    "\n",
    "print(\"Best Parameters:\", best)\n",
    "\n",
    "knn_best = KNeighborsClassifier(algorithm='auto', metric='euclidean', n_neighbors=13, weights='uniform')\n",
    "\n",
    "knn_best.fit(X_train_scaled, y_train)\n",
    "knn_best_pred = knn_best.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test, knn_best_pred))\n",
    "print(classification_report(y_test, knn_best_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Booting íŠœë‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df ë¡œë“œ\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# í•™ìŠµ, í…ŒìŠ¤íŠ¸ ë°ì´í„° ë‚˜ëˆ„ê¸°\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(df, target, test_size=0.2, random_state=42,stratify=target)\n",
    "\n",
    "\n",
    "# ìŠ¤ì¼€ì¼ëŸ¬\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ì—°ì†í˜• ë³€ìˆ˜\n",
    "continuous_cols = ['HH Income', 'Year Of Residence', 'Age', 'weekly fee', 'Deliveryperiod']\n",
    "scaler=StandardScaler()\n",
    "X_train[continuous_cols] = scaler.fit_transform(X_train[continuous_cols].astype(float))\n",
    "X_test[continuous_cols] = scaler.transform(X_test[continuous_cols].astype(float))\n",
    "\n",
    "\n",
    "\n",
    "# í•™ìŠµ ë° ì„±ëŠ¥í‰ê°€\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import numpy as np\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score,precision_score,recall_score,f1_score\n",
    "\n",
    "\n",
    "# ê°œë³„ ëª¨ë¸ë“¤\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "xgb = XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "lgbm = LGBMClassifier(random_state=42)\n",
    "\n",
    "# VotingClassifierì— ëª¨ë¸ë“¤ì„ ì¶”ê°€\n",
    "vclf = VotingClassifier(estimators=[('rf', rf), ('xgb', xgb), ('lgbm', lgbm)], voting='soft')  # ë˜ëŠ” hardë¡œ ë³€ê²½ ê°€ëŠ¥\n",
    "\n",
    "# íŒŒë¼ë¯¸í„° ì§€ì •\n",
    "params = {\n",
    "    'voting': ['soft', 'hard'],\n",
    "    # RandomForest í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "    'rf__n_estimators': randint(100, 301), \n",
    "    'rf__max_depth': [3,4,5],  \n",
    "\n",
    "    # XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "    'xgb__max_depth': [3, 4, 5], \n",
    "    'xgb__learning_rate': uniform(0.01, 0.3),\n",
    "    'xgb__reg_alpha': uniform(0.01, 0.3),  # L1 Regularization\n",
    "    'xgb__reg_lambda': uniform(0.01, 0.3), # L2 Regularization  \n",
    "    \n",
    "    # LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "    'lgbm__num_leaves': randint(20, 40),\n",
    "    'lgbm__learning_rate': uniform(0.01, 0.3),\n",
    "    'lgbm__lambda_l1': uniform(0.01, 0.3),  # L1 Regularization\n",
    "    'lgbm__lambda_l2': uniform(0.01, 0.3),  # L2 Regularization  \n",
    "\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV ì‚¬ìš©\n",
    "random_search = RandomizedSearchCV(vclf, params, n_iter=40, scoring='f1', cv=5, n_jobs=-1, random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"ìµœì ì˜ íŒŒë¼ë¯¸í„°:\", random_search.best_params_)\n",
    "print(\"ìµœì í™”ëœ f1-score:\", random_search.best_score_)\n",
    "\n",
    "# ìµœì ì˜ ëª¨ë¸ë¡œ ì˜ˆì¸¡\n",
    "y_pred = random_search.best_estimator_.predict(X_test)\n",
    "y_pred_prob=random_search.best_estimator_.predict_proba(X_test)\n",
    "\n",
    "\n",
    "print(\"Accuracy:\",accuracy_score(y_test,y_pred))\n",
    "print(\"Precision:\",precision_score(y_test,y_pred))\n",
    "print(\"Recall:\",recall_score(y_test,y_pred))\n",
    "print(\"F1-score\",f1_score(y_test,y_pred))\n",
    "\n",
    "# classification_report ì¶œë ¥\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ëœë¤í¬ë ˆìŠ¤íŠ¸ íŠœë‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, roc_auc_score, classification_report)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df, target,\n",
    "    test_size=0.2,\n",
    "    stratify=target,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "smote = SMOTE(sampling_strategy=0.5, random_state=42)\n",
    "X_res_scaled, y_res = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "rf_clf = RandomForestClassifier(\n",
    "    random_state=42,\n",
    "    class_weight={0:6, 1:1},  \n",
    "    bootstrap=True\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 300],  \n",
    "    'max_depth': [5, 7, None],  \n",
    "    'min_samples_split': [5, 10],  \n",
    "    'min_samples_leaf': [2, 4],  \n",
    "    'max_features': ['sqrt', 0.8]  \n",
    "}\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  \n",
    "grid_search_rf = GridSearchCV(\n",
    "    estimator=rf_clf,\n",
    "    param_grid=param_grid,\n",
    "    cv=skf,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=2  \n",
    ")\n",
    "\n",
    "\n",
    "grid_search_rf.fit(X_res_scaled, y_res)\n",
    "\n",
    "\n",
    "print(\"\\nğŸ† ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:\", grid_search_rf.best_params_)\n",
    "print(\"\\nğŸ” ìƒìœ„ 10ê°œ íŒŒë¼ë¯¸í„° ì¡°í•©:\")\n",
    "results_df = pd.DataFrame(grid_search_rf.cv_results_)\n",
    "print(results_df[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']]\n",
    "      .sort_values('rank_test_score')\n",
    "      .head(10))\n",
    "\n",
    "\n",
    "best_rf_model = grid_search_rf.best_estimator_\n",
    "\n",
    "def evaluate_rf(model, X_scaled, y_true, dataset_name):\n",
    "    y_pred = model.predict(X_scaled)  \n",
    "    y_proba = model.predict_proba(X_scaled)[:, 1]  \n",
    "    \n",
    "    print(f\"\\nğŸ“Š {dataset_name} ì„±ëŠ¥ í‰ê°€:\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"Recall: {recall_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"F1-Score: {f1_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_true, y_proba):.4f}\")\n",
    "    \n",
    "    print(\"\\n              precision    recall  f1-score   support\")\n",
    "    report = classification_report(y_true, y_pred, target_names=['Class 0', 'Class 1'], digits=4)\n",
    "    print(report)\n",
    "\n",
    "print(\"\\n=== ìŠ¤ì¼€ì¼ë§ ì ìš© ëª¨ë¸ ì„±ëŠ¥ ===\")\n",
    "evaluate_rf(best_rf_model, X_train_scaled, y_train, \"í›ˆë ¨ ë°ì´í„°\")\n",
    "evaluate_rf(best_rf_model, X_test_scaled, y_test, \"í…ŒìŠ¤íŠ¸ ë°ì´í„°\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN íŠœë‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt\n",
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, target, test_size=0.2, random_state=42)\n",
    "\n",
    "def objective_knn(params):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    model = KNeighborsClassifier(\n",
    "        n_neighbors=int(params['n_neighbors']),\n",
    "        weights=params['weights'],\n",
    "        metric=params['metric']\n",
    "    )\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return {'loss': -acc, 'status': hyperopt.STATUS_OK}\n",
    "\n",
    "space = {\n",
    "    'n_neighbors': hp.quniform('n_neighbors', 3, 20, 1),\n",
    "    'weights': hp.choice('weights', ['uniform', 'distance']),\n",
    "    'metric': hp.choice('metric', ['euclidean', 'manhattan']),\n",
    "    'algorithm': hp.choice('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute'])\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(\n",
    "    fn=objective_knn,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "weights_options = ['uniform', 'distance']\n",
    "metric_options = ['euclidean', 'manhattan']\n",
    "\n",
    "best['n_neighbors'] = int(best['n_neighbors'])\n",
    "best['weights'] = weights_options[best['weights']]\n",
    "best['metric'] = metric_options[best['metric']]\n",
    "\n",
    "print(\"Best Parameters:\", best)\n",
    "\n",
    "knn_best = KNeighborsClassifier(algorithm='auto', metric='euclidean', n_neighbors=13, weights='uniform')\n",
    "\n",
    "knn_best.fit(X_train_scaled, y_train)\n",
    "knn_best_pred = knn_best.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test, knn_best_pred))\n",
    "print(classification_report(y_test, knn_best_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost íŠœë‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import classification_report, f1_score, make_scorer\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res_scaled, y_res = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'depth': [6, 10],\n",
    "    'iterations': [500, 1000],\n",
    "    'learning_rate': [0.03, 0.1],\n",
    "    'l2_leaf_reg': [1, 3],\n",
    "    'class_weights': [{0:3, 1:1}]\n",
    "}\n",
    "\n",
    "\n",
    "f1_scorer = make_scorer(f1_score, pos_label=0)\n",
    "\n",
    "\n",
    "catboost = CatBoostClassifier(\n",
    "    random_seed=42,\n",
    "    eval_metric='F1',\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=0\n",
    ")\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    catboost,\n",
    "    param_grid,\n",
    "    scoring=f1_scorer,\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "\n",
    "grid_search.fit(X_res_scaled, y_res)\n",
    "\n",
    "print(f\"ìµœì  íŒŒë¼ë¯¸í„°: {grid_search.best_params_}\")\n",
    "print(f\"ìµœê³  F1-Score(0): {grid_search.best_score_:.4f}\")\n",
    "\n",
    "best_cb = grid_search.best_estimator_\n",
    "y_pred = best_cb.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\ní…ŒìŠ¤íŠ¸ ì„±ëŠ¥:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pystudy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
