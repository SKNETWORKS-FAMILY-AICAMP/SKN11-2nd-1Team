{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# 엑셀 파일 불러오기\n",
    "file_path = \"../data/NewspaperChurn.xlsx\"\n",
    "df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns drop\n",
    "df = df.drop(['SubscriptionID', 'Ethnicity', 'Language', 'Address', 'State', 'City', 'County', 'Zip Code', 'Source Channel'], axis=1)\n",
    "\n",
    "# Subscriber열 No : 1, Yes :0\n",
    "def preprocess_subscriber(df):\n",
    "    df['Subscriber'] = df['Subscriber'].map({'NO': 1, 'YES': 0})\n",
    "    return df\n",
    "    \n",
    "# reward program 열 1 이상은 1, 미만은 0, 결측치도 0\n",
    "def preprocess_reward_program(df):\n",
    "    df['reward program'] = df['reward program'].apply(lambda x: 1 if x >= 1 else 0)\n",
    "    return df\n",
    "    \n",
    "# 고객유형코드\n",
    "def preprocess_nielsen_prizm(df):\n",
    "    nielsen_mapping = {\n",
    "        'FM': 0,  # Female Middle-aged\n",
    "        'MW': 1,  # Male Working-age\n",
    "        'MM': 2,  # Male Middle-aged\n",
    "        'FW': 3,  # Female Working-age\n",
    "        'YW': 4,  # Young Woman\n",
    "        'YM': 5,  # Young Man\n",
    "        'ME': 6,  # Male Elderly\n",
    "        'FE': 7,  # Female Elderly\n",
    "        'YE': 8   # Young Elderly\n",
    "    }\n",
    "    \n",
    "    # 'Nielsen Prizm'을 매핑된 숫자로 변환\n",
    "    df['Nielsen Prizm'] = df['Nielsen Prizm'].map(nielsen_mapping)\n",
    "    \n",
    "    # 129개의 결측치는 날림 (삭제) # 이거는 나중에 삭제 ?? # 위에서 따로\n",
    "    df.dropna(subset=['Nielsen Prizm'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# 2. 근로 중 여부 (중이면 1, 아니면 0)\n",
    "def preprocess_working(df):\n",
    "    df['Working'] = df['Nielsen Prizm'].apply(lambda x: 1 if x in [1, 3] else 0)\n",
    "    return df\n",
    "\n",
    "# 3. 성별 (남 : 0, 여: 1)\n",
    "def preprocess_gender(df):\n",
    "    # 기본 성별 처리\n",
    "    df['Gender'] = df['Nielsen Prizm'].apply(lambda x: 0 if x in [1, 2, 5, 6] else 1)\n",
    "    \n",
    "    # 'YE' (Young Elderly) 처리: 25명 남성 (0), 26명 여성 (1) 랜덤으로 배분\n",
    "    ye_indices = df[df['Nielsen Prizm'] == 8].index  # 'YE'에 해당하는 인덱스 찾기\n",
    "    \n",
    "    # 랜덤 시드를 고정하여 항상 동일한 결과가 나오도록 설정\n",
    "    random.seed(42)  # 시드 값 (고정된 숫자)\n",
    "    \n",
    "    # 'YE'에 해당하는 인덱스의 길이에 맞는 gender_assignment 리스트 생성\n",
    "    gender_assignment = [0] * (len(ye_indices) // 2) + [1] * (len(ye_indices) - len(ye_indices) // 2)  # 남성 25명, 여성 26명\n",
    "    random.shuffle(gender_assignment)  # 섞기\n",
    "    \n",
    "    # YE 인덱스에 해당하는 성별 값을 할당\n",
    "    df.loc[ye_indices, 'Gender'] = gender_assignment\n",
    "    \n",
    "    return df\n",
    "\n",
    "# weekly fee\n",
    "def preprocess_fee(value):\n",
    "    if pd.isnull(value):\n",
    "        return None\n",
    "    \n",
    "    match = re.findall(r'\\d+\\.\\d+|\\d+', str(value))\n",
    "    num = [float(x) for x in match]\n",
    "\n",
    "    if len(num) == 2:\n",
    "        return (num[0] + num[1]) / 2\n",
    "    elif len(num) == 1:\n",
    "        return num[0]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def nan_fee(value):\n",
    "    avg_fee = np.mean(df['weekly fee'])\n",
    "    if pd.isnull(value):\n",
    "        return avg_fee\n",
    "    else:\n",
    "        return value\n",
    "        \n",
    "# Deliveryperiod 전처리\n",
    "def preprocess_deliveryperiod(df):\n",
    "    # 온라인 배송 타입 정의\n",
    "    online = {\n",
    "        '7DayT', 'Fri-SunT', 'Sun-FriT', 'Thu-SunT', \n",
    "        'SoooTFST', 'Fri-SunT', 'SooooFST', 'SoooooST', 'SunOnlyT', 'SooooooT'\n",
    "    }\n",
    "\n",
    "\n",
    "    period_map = {\n",
    "        '7Day': 7, '7DAY': 7, '7day': 7, '7DayOL': 7, '7DayT': 7,\n",
    "        'Fri-Sun': 6, 'Fri-SunT': 6, 'Sun-Fri': 6, 'Sun-FriT': 6,\n",
    "        'oMTWTFo': 5, 'Mon-Fri': 5,\n",
    "        'Thu-Sun': 4, 'THU-SUN': 4, 'thu-sun': 4, 'SoooTFS': 4, 'SoooTFST': 4, 'Thu-SunT': 4,\n",
    "        'SooooFS': 3, 'SooooFST': 3, 'Fri-SunT': 3, 'Fri-Sun': 3,\n",
    "        'SatSun': 2, 'SoooooS': 2, 'SoooooST': 2,\n",
    "        'SunOnly': 1, 'SunOnlyT': 1, 'sunonly': 1, 'SUNONLY': 1, 'Soooooo': 1, 'SooooooT': 1\n",
    "    }\n",
    "\n",
    "    df['Is_Online'] = df['Deliveryperiod'].apply(lambda x: 1 if x in online else 0)\n",
    "    df['Deliveryperiod'] = df['Deliveryperiod'].map(period_map)\n",
    "\n",
    "    return df\n",
    "\n",
    "            \n",
    "# dummy for children (자녀가 있으면 1, 없으면 0)       \n",
    "def preprocess_dummy(df):\n",
    "    df['dummy for Children'] = np.where(df['dummy for Children'] == 'Y', 1, 0)\n",
    "    return df\n",
    "    \n",
    "    \n",
    "#hh income 전처리\n",
    "def preprocess_hh_income(df):\n",
    "    \"\"\"\n",
    "    HH Income 열을 전처리하여:\n",
    "    1. `plus`나 `under`가 포함된 값을 평균값으로 대체\n",
    "    2. 범위 데이터(`$30,000 - $39,999`)는 평균값으로 변환\n",
    "    3. 기호(`$`, `,`) 제거\n",
    "    \"\"\"\n",
    "    # 1. \"plus\" 또는 \"under\"가 포함된 행을 마스킹\n",
    "    mask = df['HH Income'].str.contains('plus|under', case=False, na=False)\n",
    "    \n",
    "    # 2. 범위 데이터를 평균값으로 변환 (plus/under 제외)\n",
    "    def process_value(value):\n",
    "        try:\n",
    "            # 기호 제거\n",
    "            clean_value = str(value).replace('$', '').replace(',', '')\n",
    "            \n",
    "            # 범위 처리 (예: \"30000-39999\")\n",
    "            if '-' in clean_value:\n",
    "                low, high = map(int, clean_value.split('-'))\n",
    "                return (low + high) / 2\n",
    "            # 숫자 처리\n",
    "            elif clean_value.isdigit():\n",
    "                return float(clean_value)\n",
    "            else:\n",
    "                return np.nan  # plus/under는 NaN 처리\n",
    "        except:\n",
    "            return np.nan\n",
    "    \n",
    "    # 3. 전체 열을 숫자로 변환 (plus/under는 NaN)\n",
    "    df['HH Income_Processed'] = df['HH Income'].apply(process_value)\n",
    "    \n",
    "    # 4. 평균값 계산 (plus/under 제외)\n",
    "    valid_mean = df.loc[~mask, 'HH Income_Processed'].mean()\n",
    "    \n",
    "    # 5. plus/under가 포함된 행을 평균값으로 대체\n",
    "    df.loc[mask, 'HH Income_Processed'] = valid_mean\n",
    "    \n",
    "    # 6. 원본 열 대체\n",
    "    df['HH Income'] = df['HH Income_Processed'].round(0).astype(int)\n",
    "    df = df.drop('HH Income_Processed', axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "#Home Ownership 전처리\n",
    "def preprocess_home_ownership(df):\n",
    "    df['Home Ownership'] = df['Home Ownership'].map({'OWNER': 1, 'RENTER': 0})\n",
    "    return df\n",
    "\n",
    "def preprocess_age(value):\n",
    "    if pd.isnull(value):\n",
    "        return None\n",
    "    \n",
    "    match = re.findall(r'\\d[\\d,]*', str(value))\n",
    "    num = [int(x.replace(',', '')) for x in match]\n",
    "\n",
    "    if len(num) == 2:\n",
    "        return (num[0] + num[1]) / 2\n",
    "    elif len(num) == 1:\n",
    "        return num[0]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def nan_age(value):\n",
    "    avg_age = np.mean(df['Age range'])\n",
    "    if pd.isnull(value):\n",
    "        return avg_age\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "df['Age range'] = df['Age range'].apply(preprocess_age)\n",
    "df['Age range'] = df['Age range'].apply(nan_age)\n",
    "df['Age range'] = np.floor(df['Age range']).astype(int)\n",
    "df.rename(columns={'Age range': 'Age'}, inplace=True)\n",
    "df = preprocess_subscriber(df)\n",
    "df = preprocess_reward_program(df)\n",
    "df = preprocess_nielsen_prizm(df)\n",
    "df = preprocess_working(df)\n",
    "df = preprocess_gender(df)\n",
    "df['weekly fee'] = df['weekly fee'].apply(preprocess_fee)\n",
    "df['weekly fee'] = df['weekly fee'].apply(nan_fee)\n",
    "df = preprocess_deliveryperiod(df)\n",
    "df = preprocess_dummy(df)\n",
    "df = preprocess_hh_income(df)\n",
    "df = preprocess_home_ownership(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 타깃 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['Subscriber']\n",
    "df = df.drop('Subscriber', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모든 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[로지스틱 회귀 (Cost-Sensitive)]\n",
      "Accuracy: 0.6809\n",
      "Precision: 0.8771\n",
      "Recall: 0.7084\n",
      "F1 Score: 0.7838\n",
      "AUC-ROC: 0.6333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.56      0.39       577\n",
      "           1       0.88      0.71      0.78      2569\n",
      "\n",
      "    accuracy                           0.68      3146\n",
      "   macro avg       0.59      0.63      0.59      3146\n",
      "weighted avg       0.77      0.68      0.71      3146\n",
      "\n",
      "\n",
      "[KNN]\n",
      "Accuracy: 0.6771\n",
      "Precision: 0.8916\n",
      "Recall: 0.6882\n",
      "F1 Score: 0.7768\n",
      "AUC-ROC: 0.6578\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.63      0.42       577\n",
      "           1       0.89      0.69      0.78      2569\n",
      "\n",
      "    accuracy                           0.68      3146\n",
      "   macro avg       0.60      0.66      0.60      3146\n",
      "weighted avg       0.79      0.68      0.71      3146\n",
      "\n",
      "\n",
      "[랜덤 포레스트 (Cost-Sensitive)]\n",
      "Accuracy: 0.8147\n",
      "Precision: 0.8796\n",
      "Recall: 0.8957\n",
      "F1 Score: 0.8876\n",
      "AUC-ROC: 0.6749\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.45      0.47       577\n",
      "           1       0.88      0.90      0.89      2569\n",
      "\n",
      "    accuracy                           0.81      3146\n",
      "   macro avg       0.69      0.67      0.68      3146\n",
      "weighted avg       0.81      0.81      0.81      3146\n",
      "\n",
      "\n",
      "[XGBoost (Cost-Sensitive)]\n",
      "Accuracy: 0.8433\n",
      "Precision: 0.8460\n",
      "Recall: 0.9879\n",
      "F1 Score: 0.9115\n",
      "AUC-ROC: 0.5936\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.20      0.32       577\n",
      "           1       0.85      0.99      0.91      2569\n",
      "\n",
      "    accuracy                           0.84      3146\n",
      "   macro avg       0.82      0.59      0.61      3146\n",
      "weighted avg       0.84      0.84      0.80      3146\n",
      "\n",
      "\n",
      "[다중 퍼셉트론 (MLP, Cost-Sensitive)]\n",
      "Accuracy: 0.7285\n",
      "Precision: 0.8830\n",
      "Recall: 0.7696\n",
      "F1 Score: 0.8224\n",
      "AUC-ROC: 0.6577\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.55      0.42       577\n",
      "           1       0.88      0.77      0.82      2569\n",
      "\n",
      "    accuracy                           0.73      3146\n",
      "   macro avg       0.62      0.66      0.62      3146\n",
      "weighted avg       0.78      0.73      0.75      3146\n",
      "\n",
      "\n",
      "[SVM (Cost-Sensitive)]\n",
      "Accuracy: 0.6964\n",
      "Precision: 0.8895\n",
      "Recall: 0.7174\n",
      "F1 Score: 0.7942\n",
      "AUC-ROC: 0.6603\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.60      0.42       577\n",
      "           1       0.89      0.72      0.79      2569\n",
      "\n",
      "    accuracy                           0.70      3146\n",
      "   macro avg       0.61      0.66      0.61      3146\n",
      "weighted avg       0.79      0.70      0.73      3146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, target, test_size=0.2, random_state=0)\n",
    "\n",
    "# 1. 데이터 정규화\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Smote\n",
    "smote_tomek = SMOTE(random_state=0)\n",
    "X_train_scaled, y_train= smote_tomek.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# 3. 모델 학습 및 평가 함수 정의\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"AUC-ROC: {roc_auc:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 4. 로지스틱 회귀 (가중치 추가)\n",
    "print(\"\\n[로지스틱 회귀 (Cost-Sensitive)]\")\n",
    "lr_model = LogisticRegression(random_state=0, class_weight='balanced')\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "evaluate_model(lr_model, X_test_scaled, y_test)\n",
    "\n",
    "# 5. KNN 모델\n",
    "print(\"\\n[KNN]\")\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train_scaled, y_train)\n",
    "evaluate_model(knn_model, X_test_scaled, y_test)\n",
    "\n",
    "# 6. 랜덤 포레스트 (가중치 추가)\n",
    "print(\"\\n[랜덤 포레스트 (Cost-Sensitive)]\")\n",
    "rf_model = RandomForestClassifier(random_state=0, class_weight='balanced')\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "evaluate_model(rf_model, X_test_scaled, y_test)\n",
    "\n",
    "# 7. XGBoost 모델 (가중치 추가)\n",
    "print(\"\\n[XGBoost (Cost-Sensitive)]\")\n",
    "xgb_model = XGBClassifier(random_state=0, eval_metric='logloss', scale_pos_weight=4)\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "evaluate_model(xgb_model, X_test_scaled, y_test)\n",
    "\n",
    "# 8. 다중 퍼셉트론 (MLPClassifier, 가중치 추가)\n",
    "print(\"\\n[다중 퍼셉트론 (MLP, Cost-Sensitive)]\")\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=300, random_state=0)\n",
    "mlp_model.fit(X_train_scaled, y_train)\n",
    "evaluate_model(mlp_model, X_test_scaled, y_test)\n",
    "\n",
    "# 9. SVM (가중치 추가)\n",
    "print(\"\\n[SVM (Cost-Sensitive)]\")\n",
    "svm_model = SVC(kernel='rbf', probability=True, class_weight='balanced', random_state=0)\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "evaluate_model(svm_model, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, target, test_size=0.2, random_state=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_resample_train_scaled = scaler.fit_transform(X_train)\n",
    "X_resample_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resample, y_resample = smote.fit_resample(X_resample_train_scaled, y_train)\n",
    "\n",
    "# MLPClassifier를 위한 하이퍼파라미터 그리드\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(64, 32, 16), (128, 64, 32), (32, 16, 8)],  # 은닉층 구조 다양화\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],   \n",
    "    'learning_rate_init': [0.001, 0.005, 0.01],  # 학습률 튜닝\n",
    "    'alpha': [0.0001, 0.001, 0.01],  # 정규화 강도\n",
    "    'batch_size': [32, 64, 128],  # 미니 배치 크기 조정\n",
    "    'max_iter': [500, 1000],  # 에포크 증가\n",
    "}\n",
    "\n",
    "# 모델 정의\n",
    "mlp = MLPClassifier(early_stopping=True, random_state=0)\n",
    "\n",
    "# 그리드 서치 설정\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "grid_search = GridSearchCV(mlp, param_grid, scoring='f1_weighted', cv=cv, n_jobs=-1, verbose=2)\n",
    "\n",
    "# 모델 학습 및 최적 파라미터 탐색\n",
    "grid_search.fit(X_resample, y_resample)\n",
    "\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best F1-Score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# 최적 모델로 예측\n",
    "best_mlp = grid_search.best_estimator_\n",
    "y_pred = best_mlp.predict(X_resample_test_scaled)\n",
    "\n",
    "# 평가\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(df,target, test_size=0.2, random_state=0)\n",
    "\n",
    "# 데이터 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# SVM 하이퍼파라미터 그리드\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],               # 규제 강도\n",
    "    'kernel': ['linear', 'rbf', 'poly'],  # 커널 함수\n",
    "    'gamma': ['scale', 'auto'],           # 감마 설정\n",
    "    'degree': [2, 3],                     # 다항 커널 차수\n",
    "}\n",
    "\n",
    "# 모델 정의\n",
    "svc = SVC(random_state=0)\n",
    "\n",
    "# 그리드 서치 설정\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "grid_search = GridSearchCV(svc, param_grid, scoring='f1_weighted', cv=cv, n_jobs=-1, verbose=2)\n",
    "\n",
    "# 모델 학습 및 최적 파라미터 탐색\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best F1-Score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# 최적 모델로 예측\n",
    "best_svc = grid_search.best_estimator_\n",
    "y_pred = best_svc.predict(X_test_scaled)\n",
    "\n",
    "# 평가\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# 데이터 스케일링\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# SMOTE로 데이터 증강\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resample, y_resample = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# 1. 기본 Gradient Boosting 모델\n",
    "gbc = GradientBoostingClassifier(random_state=42)\n",
    "gbc.fit(X_resample, y_resample)\n",
    "y_pred_gbc = gbc.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\n[Gradient Boosting 기본 모델 평가]\")\n",
    "print(\"정확도:\", accuracy_score(y_test, y_pred_gbc))\n",
    "print(classification_report(y_test, y_pred_gbc))\n",
    "\n",
    "# 2. 하이퍼파라미터 튜닝 (GridSearchCV)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(GradientBoostingClassifier(random_state=42),\n",
    "                           param_grid, scoring='f1_weighted', cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_resample, y_resample)\n",
    "\n",
    "print(\"\\n[Gradient Boosting 최적 파라미터]\")\n",
    "print(grid_search.best_params_)\n",
    "best_gbc = grid_search.best_estimator_\n",
    "\n",
    "# 최적 모델로 예측\n",
    "y_pred_best_gbc = best_gbc.predict(X_test_scaled)\n",
    "print(\"\\n[최적 Gradient Boosting 모델 평가]\")\n",
    "print(\"정확도:\", accuracy_score(y_test, y_pred_best_gbc))\n",
    "print(classification_report(y_test, y_pred_best_gbc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필수 라이브러리 임포트\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, target, test_size=0.2, random_state=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_resample_train_scaled = scaler.fit_transform(X_train)\n",
    "X_resample_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_resample_train_scaled, y_train)  # Fix: Resample both X_train and y_train together\n",
    "\n",
    "\n",
    "# 4. LightGBM 모델 설정\n",
    "lgbm = lgb.LGBMClassifier(random_state=0, is_unbalance=True)\n",
    "\n",
    "# 5. 하이퍼파라미터 그리드 설정\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],        # 트리 개수\n",
    "    'learning_rate': [0.01, 0.05, 0.1],     # 학습률\n",
    "    'num_leaves': [31, 63, 127],            # 리프 노드 수\n",
    "    'max_depth': [-1, 10, 20],              # 트리 깊이\n",
    "    'scale_pos_weight': [1, 10, 20]         # 불균형 데이터 보정\n",
    "}\n",
    "\n",
    "# 6. 교차 검증 설정 (Stratified K-Fold 사용)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# 7. 그리드 서치 설정\n",
    "grid_search = GridSearchCV(\n",
    "    lgbm, param_grid, scoring='f1_weighted', \n",
    "    cv=cv, n_jobs=-1, verbose=2\n",
    ")\n",
    "\n",
    "# 8. 모델 학습\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 10. 최적 모델로 예측\n",
    "best_lgbm = grid_search.best_estimator_\n",
    "y_pred = best_lgbm.predict(X_resample_test_scaled)\n",
    "\n",
    "# 11. 모델 평가\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt\n",
    "import hyperopt.hp\n",
    "\n",
    "def objective_knn(params):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    model = KNeighborsClassifier(\n",
    "        n_neighbors=int(params['n_neighbors']),\n",
    "        weights=params['weights'],\n",
    "        metric=params['metric']\n",
    "    )\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return {'loss': -acc, 'status': hyperopt.STATUS_OK}\n",
    "\n",
    "space = {\n",
    "    'n_neighbors': hp.quniform('n_neighbors', 3, 20, 1),\n",
    "    'weights': hp.choice('weights', ['uniform', 'distance']),\n",
    "    'metric': hp.choice('metric', ['euclidean', 'manhattan']),\n",
    "    'algorithm': hp.choice('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute'])\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(\n",
    "    fn=objective_knn,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "weights_options = ['uniform', 'distance']\n",
    "metric_options = ['euclidean', 'manhattan']\n",
    "\n",
    "best['n_neighbors'] = int(best['n_neighbors'])\n",
    "best['weights'] = weights_options[best['weights']]\n",
    "best['metric'] = metric_options[best['metric']]\n",
    "\n",
    "print(\"Best Parameters:\", best)\n",
    "\n",
    "knn_best = KNeighborsClassifier(algorithm='auto', metric='euclidean', n_neighbors=13, weights='uniform')\n",
    "\n",
    "knn_best.fit(X_train_scaled, y_train)\n",
    "knn_best_pred = knn_best.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test, knn_best_pred))\n",
    "print(classification_report(y_test, knn_best_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Booting 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df 로드\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 학습, 테스트 데이터 나누기\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(df, target, test_size=0.2, random_state=42,stratify=target)\n",
    "\n",
    "\n",
    "# 스케일러\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 연속형 변수\n",
    "continuous_cols = ['HH Income', 'Year Of Residence', 'Age', 'weekly fee', 'Deliveryperiod']\n",
    "scaler=StandardScaler()\n",
    "X_train[continuous_cols] = scaler.fit_transform(X_train[continuous_cols].astype(float))\n",
    "X_test[continuous_cols] = scaler.transform(X_test[continuous_cols].astype(float))\n",
    "\n",
    "\n",
    "\n",
    "# 학습 및 성능평가\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import numpy as np\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score,precision_score,recall_score,f1_score\n",
    "\n",
    "\n",
    "# 개별 모델들\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "xgb = XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "lgbm = LGBMClassifier(random_state=42)\n",
    "\n",
    "# VotingClassifier에 모델들을 추가\n",
    "vclf = VotingClassifier(estimators=[('rf', rf), ('xgb', xgb), ('lgbm', lgbm)], voting='soft')  # 또는 hard로 변경 가능\n",
    "\n",
    "# 파라미터 지정\n",
    "params = {\n",
    "    'voting': ['soft', 'hard'],\n",
    "    # RandomForest 하이퍼파라미터\n",
    "    'rf__n_estimators': randint(100, 301), \n",
    "    'rf__max_depth': [3,4,5],  \n",
    "\n",
    "    # XGBoost 하이퍼파라미터\n",
    "    'xgb__max_depth': [3, 4, 5], \n",
    "    'xgb__learning_rate': uniform(0.01, 0.3),\n",
    "    'xgb__reg_alpha': uniform(0.01, 0.3),  # L1 Regularization\n",
    "    'xgb__reg_lambda': uniform(0.01, 0.3), # L2 Regularization  \n",
    "    \n",
    "    # LightGBM 하이퍼파라미터\n",
    "    'lgbm__num_leaves': randint(20, 40),\n",
    "    'lgbm__learning_rate': uniform(0.01, 0.3),\n",
    "    'lgbm__lambda_l1': uniform(0.01, 0.3),  # L1 Regularization\n",
    "    'lgbm__lambda_l2': uniform(0.01, 0.3),  # L2 Regularization  \n",
    "\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV 사용\n",
    "random_search = RandomizedSearchCV(vclf, params, n_iter=40, scoring='f1', cv=5, n_jobs=-1, random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"최적의 파라미터:\", random_search.best_params_)\n",
    "print(\"최적화된 f1-score:\", random_search.best_score_)\n",
    "\n",
    "# 최적의 모델로 예측\n",
    "y_pred = random_search.best_estimator_.predict(X_test)\n",
    "y_pred_prob=random_search.best_estimator_.predict_proba(X_test)\n",
    "\n",
    "\n",
    "print(\"Accuracy:\",accuracy_score(y_test,y_pred))\n",
    "print(\"Precision:\",precision_score(y_test,y_pred))\n",
    "print(\"Recall:\",recall_score(y_test,y_pred))\n",
    "print(\"F1-score\",f1_score(y_test,y_pred))\n",
    "\n",
    "# classification_report 출력\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 랜덤포레스트 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, roc_auc_score, classification_report)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df, target,\n",
    "    test_size=0.2,\n",
    "    stratify=target,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "smote = SMOTE(sampling_strategy=0.5, random_state=42)\n",
    "X_res_scaled, y_res = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "rf_clf = RandomForestClassifier(\n",
    "    random_state=42,\n",
    "    class_weight={0:6, 1:1},  \n",
    "    bootstrap=True\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 300],  \n",
    "    'max_depth': [5, 7, None],  \n",
    "    'min_samples_split': [5, 10],  \n",
    "    'min_samples_leaf': [2, 4],  \n",
    "    'max_features': ['sqrt', 0.8]  \n",
    "}\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  \n",
    "grid_search_rf = GridSearchCV(\n",
    "    estimator=rf_clf,\n",
    "    param_grid=param_grid,\n",
    "    cv=skf,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=2  \n",
    ")\n",
    "\n",
    "\n",
    "grid_search_rf.fit(X_res_scaled, y_res)\n",
    "\n",
    "\n",
    "print(\"\\n🏆 최적 하이퍼파라미터:\", grid_search_rf.best_params_)\n",
    "print(\"\\n🔍 상위 10개 파라미터 조합:\")\n",
    "results_df = pd.DataFrame(grid_search_rf.cv_results_)\n",
    "print(results_df[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']]\n",
    "      .sort_values('rank_test_score')\n",
    "      .head(10))\n",
    "\n",
    "\n",
    "best_rf_model = grid_search_rf.best_estimator_\n",
    "\n",
    "def evaluate_rf(model, X_scaled, y_true, dataset_name):\n",
    "    y_pred = model.predict(X_scaled)  \n",
    "    y_proba = model.predict_proba(X_scaled)[:, 1]  \n",
    "    \n",
    "    print(f\"\\n📊 {dataset_name} 성능 평가:\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"Recall: {recall_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"F1-Score: {f1_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_true, y_proba):.4f}\")\n",
    "    \n",
    "    print(\"\\n              precision    recall  f1-score   support\")\n",
    "    report = classification_report(y_true, y_pred, target_names=['Class 0', 'Class 1'], digits=4)\n",
    "    print(report)\n",
    "\n",
    "print(\"\\n=== 스케일링 적용 모델 성능 ===\")\n",
    "evaluate_rf(best_rf_model, X_train_scaled, y_train, \"훈련 데이터\")\n",
    "evaluate_rf(best_rf_model, X_test_scaled, y_test, \"테스트 데이터\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt\n",
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, target, test_size=0.2, random_state=42)\n",
    "\n",
    "def objective_knn(params):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    model = KNeighborsClassifier(\n",
    "        n_neighbors=int(params['n_neighbors']),\n",
    "        weights=params['weights'],\n",
    "        metric=params['metric']\n",
    "    )\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return {'loss': -acc, 'status': hyperopt.STATUS_OK}\n",
    "\n",
    "space = {\n",
    "    'n_neighbors': hp.quniform('n_neighbors', 3, 20, 1),\n",
    "    'weights': hp.choice('weights', ['uniform', 'distance']),\n",
    "    'metric': hp.choice('metric', ['euclidean', 'manhattan']),\n",
    "    'algorithm': hp.choice('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute'])\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(\n",
    "    fn=objective_knn,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "weights_options = ['uniform', 'distance']\n",
    "metric_options = ['euclidean', 'manhattan']\n",
    "\n",
    "best['n_neighbors'] = int(best['n_neighbors'])\n",
    "best['weights'] = weights_options[best['weights']]\n",
    "best['metric'] = metric_options[best['metric']]\n",
    "\n",
    "print(\"Best Parameters:\", best)\n",
    "\n",
    "knn_best = KNeighborsClassifier(algorithm='auto', metric='euclidean', n_neighbors=13, weights='uniform')\n",
    "\n",
    "knn_best.fit(X_train_scaled, y_train)\n",
    "knn_best_pred = knn_best.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test, knn_best_pred))\n",
    "print(classification_report(y_test, knn_best_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import classification_report, f1_score, make_scorer\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res_scaled, y_res = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'depth': [6, 10],\n",
    "    'iterations': [500, 1000],\n",
    "    'learning_rate': [0.03, 0.1],\n",
    "    'l2_leaf_reg': [1, 3],\n",
    "    'class_weights': [{0:3, 1:1}]\n",
    "}\n",
    "\n",
    "\n",
    "f1_scorer = make_scorer(f1_score, pos_label=0)\n",
    "\n",
    "\n",
    "catboost = CatBoostClassifier(\n",
    "    random_seed=42,\n",
    "    eval_metric='F1',\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=0\n",
    ")\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    catboost,\n",
    "    param_grid,\n",
    "    scoring=f1_scorer,\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "\n",
    "grid_search.fit(X_res_scaled, y_res)\n",
    "\n",
    "print(f\"최적 파라미터: {grid_search.best_params_}\")\n",
    "print(f\"최고 F1-Score(0): {grid_search.best_score_:.4f}\")\n",
    "\n",
    "best_cb = grid_search.best_estimator_\n",
    "y_pred = best_cb.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\n테스트 성능:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pystudy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
